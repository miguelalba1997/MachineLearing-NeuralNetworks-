{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NumberGuesser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelalba1997/MachineLearing-NeuralNetworks-/blob/master/NumberGuesser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G6wYNzojGPN",
        "colab_type": "code",
        "outputId": "0b8c37c4-52f6-49a0-caf1-360e91adad2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
        "\n",
        "#set up the amount of nodes in each layer\n",
        "n_nodes_hl1 = 500\n",
        "n_nodes_hl2 = 500\n",
        "n_nodes_hl3 = 500\n",
        "n_nodes_ol = 10\n",
        "\n",
        "batch_size = 100 #how many images will go through the network at a time before \n",
        "                 #any updates are made to the network wieghts and biases\n",
        "\"\"\"                \n",
        "set up the height of our data. this just makes sure that the data we feed into\n",
        "our network is in the right format. input validation in a way\n",
        "\"\"\"\n",
        "x = tf.placeholder(\"float\", [None, 784]) #data\n",
        "y = tf.placeholder(\"float\") #label of our data\n",
        "\n",
        "def neural_network(data):\n",
        "    #initialize our hidden and output layers with random wieghts and biases\n",
        "    hidden_layer1 = {\"wieghts\":tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
        "                     \"biases\":tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "\n",
        "    hidden_layer2 = {\"wieghts\":tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
        "                     \"biases\":tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "\n",
        "    hidden_layer3 = {\"wieghts\":tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
        "                     \"biases\":tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
        "\n",
        "    output_layer =  {\"wieghts\":tf.Variable(tf.random_normal([n_nodes_hl3, n_nodes_ol])),\n",
        "                     \"biases\":tf.Variable(tf.random_normal([n_nodes_ol]))}\n",
        "\n",
        "    #relu == activation function. rectified linear\n",
        "    layer1 = tf.add(tf.matmul(data, hidden_layer1[\"wieghts\"]), hidden_layer1[\"biases\"])\n",
        "    layer1 = tf.nn.relu(layer1)\n",
        "\n",
        "    layer2 = tf.add(tf.matmul(layer1, hidden_layer2[\"wieghts\"]), hidden_layer2[\"biases\"])\n",
        "    layer2 = tf.nn.relu(layer2)\n",
        "\n",
        "    layer3 = tf.add(tf.matmul(layer2, hidden_layer3[\"wieghts\"]), hidden_layer3[\"biases\"])\n",
        "    layer3 = tf.nn.relu(layer3)\n",
        "\n",
        "    output = tf.add(tf.matmul(layer3, output_layer[\"wieghts\"]), output_layer[\"biases\"])\n",
        "    \n",
        "\n",
        "    return output\n",
        "\n",
        "def training(x):\n",
        "    prediction = neural_network(x)#make our network give us a prediction\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))#initiate our cost fuction\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)#stochastic gradient descent\n",
        "    \n",
        "    #set how many cycles we want to run our model for\n",
        "    epochs = 8\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "                _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
        "                loss += c\n",
        "            print(\"Epoch:\", epoch, \"Loss for Epoch\", loss)\n",
        "            \n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
        "        print(\"Accuracy:\", accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
        "        test = neural_network(mnist.test.images[0].reshape(1,-1))\n",
        "        print(\"Guess:\", tf.equal(tf.argmax(test, 1), tf.argmax(mnist.test.labels[0], 1)))\n",
        "\n",
        "print(len(mnist.test.images[0]))\n",
        "print(mnist.test.labels[0])\n",
        "plt.imshow(mnist.test.images[0].reshape(-1, 28), cmap = plt.cm.gray_r, interpolation=\"nearest\")\n",
        "plt.show()\n",
        "training(x)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "784\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5fbVYvAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss for Epoch 1788206.6568908691\n",
            "Epoch: 1 Loss for Epoch 415165.47537612915\n",
            "Epoch: 2 Loss for Epoch 224169.77331018448\n",
            "Epoch: 3 Loss for Epoch 131491.24304652214\n",
            "Epoch: 4 Loss for Epoch 80581.29843813181\n",
            "Epoch: 5 Loss for Epoch 54196.72907906771\n",
            "Epoch: 6 Loss for Epoch 33276.235056102276\n",
            "Epoch: 7 Loss for Epoch 25098.973801667304\n",
            "Accuracy: 0.9453\n",
            "Guess: Tensor(\"Equal_5:0\", shape=(1,), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}